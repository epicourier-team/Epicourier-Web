# Recipe Data Processing with Local LLaMA

This project processes recipe data using a local Ollama LLaMA model and converts it into structured JSON format.

## 📋 Prerequisites

### 1. Download Data
Download the `recipes.csv` file from the following link and save it to your project root directory:
```
https://gist.github.com/SeojinSeojin/e09119e728826ff07e5f9ba4d39a4648
```

### 2. Install and Run Ollama
You need to install Ollama locally and run it on port 11434:

```bash
# Install Ollama (https://ollama.ai)
# After installation, download the model
ollama pull llama3:instruct

# Run Ollama server (runs on port 11434 by default)
ollama serve
```

### 3. Install Python Packages

**requirements.txt:**
```txt
requests>=2.31.0
```

Installation command:
```bash
pip install -r requirements.txt
```

## 📁 Project Structure

Before running, you need the following directory structure:

```
project_root/
├── llama_recipe_pipeline.py          # Main script
├── recipes.csv                   # Recipe data (needs to be downloaded)
├── requirements.txt              # Python dependencies
└── prompts/
    ├── system_prompt.txt         # System prompt
    └── user_prompt.txt           # User prompt template
```

### Example Prompt Files

**prompts/system_prompt.txt:**
```txt
You are a helpful assistant that processes recipe data and returns structured JSON output.
Always respond with valid JSON format only, without any additional text.
```

**prompts/user_prompt.txt:**
```txt
Process the following recipe information and return a structured JSON:

Recipe Name: {strMeal}
Category: {strCategory}
Area: {strArea}
Tags: {strTags}
Instructions: {strInstructions}
Thumbnail: {strMealThumb}
Source: {strSource}
YouTube: {strYoutube}

Ingredients:
{ingredient_list}

Return a JSON with appropriate fields for this recipe.
```

## 🚀 How to Run

```bash
python llama_recipe_pipeline.py
```

## 📤 Output

After execution, the following output will be generated:

### Generated Directories and Files

```
project_root/
└── cache/
    ├── 52768.json    # Processed JSON file for each recipe ID
    ├── 52772.json
    ├── 52773.json
    └── ...
```

- **cache/** directory: Automatically created, stores the processed results for each recipe
- **File naming format**: `{recipe_id}.json` (e.g., `52768.json`)
- **File contents**: Structured recipe data generated by the LLaMA model (JSON format)

### Caching Behavior
- Already processed recipes are loaded from the cache directory and not reprocessed
- Reduces processing time and saves API calls

### Turn Cache into Supabase-Injectable csv

```bash
python cache_to_csv.py
```

## ⚙️ Configuration

You can modify the settings at the top of the script:

```python
MODEL_NAME = "llama3:instruct"   # Ollama model to use
CACHE_DIR = Path("cache")        # Cache storage directory
CSV_FILE = "recipes.csv"         # Input CSV filename
```

## 🔍 Troubleshooting

1. **Ollama Connection Failed**
   - Check if Ollama is running on port 11434: `curl http://localhost:11434`
   - Verify the model is downloaded: `ollama list`

2. **JSON Parsing Error**
   - The LLaMA model's response may not be valid JSON
   - When errors occur, the raw response is saved to the cache file and a warning is displayed in the logs

3. **Prompt Files Missing**
   - You need to create the `prompts/` directory and required prompt files

## 📝 Notes

- Each recipe processing may take several seconds depending on the LLaMA model's response time
- Retries up to 3 times on network errors
- All output is saved in UTF-8 encoding